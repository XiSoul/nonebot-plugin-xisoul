"""Ollamaäº‘æ¨¡å‹èŠå¤©æ ¸å¿ƒåŠŸèƒ½ - ä»…æä¾›åŠŸèƒ½å‡½æ•°ï¼Œå‘½ä»¤å·²ç§»è‡³__init__.py"""

import json
import asyncio
from nonebot import get_driver, logger

# å¯¼å…¥Ollamaçš„Pythonå®¢æˆ·ç«¯åº“
try:
    from ollama import Client
except ImportError:
    logger.error("æœªå®‰è£…ollama Pythonåº“ï¼Œè¯·è¿è¡Œ: pip install ollama")
    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„Clientç±»
    class Client:
        def __init__(self, host=None, headers=None):
            self.host = host
            self.headers = headers
        def chat(self, model, messages, stream=False):
            raise ImportError("æœªå®‰è£…ollama Pythonåº“")

# å¯¼å…¥å¿…è¦çš„NoneBotç±»å‹
try:
    from nonebot.adapters.onebot.v11 import Bot, Event
except ImportError:
    logger.warning("æœªå¯¼å…¥Botå’ŒEventç±»å‹ï¼Œå°†ä½¿ç”¨åŠ¨æ€ç±»å‹")
    Bot = None
    Event = None

# ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
config = get_driver().config
ollama_api_key = getattr(config, "ollama_api_key", "")

# Ollama APIä¸»æœºåœ°å€
OLLAMA_HOST = "https://ollama.com"

# åˆå§‹åŒ–å¿…è¦çš„å…¨å±€å˜é‡
conversation_histories = {}
DEFAULT_MODEL = "qwen3-coder:480b-cloud"
current_model = DEFAULT_MODEL

# å¯ç”¨æ¨¡å‹åˆ—è¡¨
available_models = [
    {"name": "qwen3-coder:480b-cloud", "chinese_name": "åƒé—®", "description": "é«˜æ€§èƒ½ä¸­æ–‡ç¼–ç æ¨¡å‹"},
    {"name": "gpt-oss:120b-cloud", "chinese_name": "GPT", "description": "é€šç”¨è¯­è¨€æ¨¡å‹"},
    {"name": "deepseek-v3.1:671b-cloud", "chinese_name": "DeepSeek", "description": "ä¸“ä¸šç¼–ç¨‹æ¨¡å‹"}
]

def is_ai_prefix(message: str) -> bool:
    """æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦ä»¥"ai+ç©ºæ ¼"å¼€å¤´"""
    return message.strip().lower().startswith("ai ")

# è¿™ä¸ªå‡½æ•°å·²è¢«ä¸‹é¢çš„æ–°ç‰ˆæœ¬æ›¿ä»£ï¼Œä¿ç•™æ³¨é‡Š

logger.info("[Ollama] æ ¸å¿ƒåŠŸèƒ½åŠ è½½å®Œæˆï¼Œå‘½ä»¤å·²ç§»è‡³__init__.py")

# æ³¨æ„ï¼šå‘½ä»¤å¤„ç†å‡½æ•°å·²ç§»è‡³__init__.py
# ä»¥ä¸‹ä¸ºåŠŸèƒ½å‡½æ•°ï¼Œä¾›__init__.pyä¸­çš„å‘½ä»¤å¤„ç†å™¨è°ƒç”¨
async def handle_model_list(bot, event):
    """æ˜¾ç¤ºå¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    response = "ğŸ“‹ **å¯ç”¨æ¨¡å‹åˆ—è¡¨**\n\n"
    for i, model in enumerate(available_models, 1):
        is_current = " âœ…" if model["name"] == current_model else ""
        response += f"{i}. {model['chinese_name']} ({model['name']}){is_current}\n"
        response += f"   ç®€ä»‹: {model['description']}\n\n"
    await bot.send(event, response)

async def handle_ollama_help(bot, event):
    """æ˜¾ç¤ºOllamaèŠå¤©æ’ä»¶çš„å¸®åŠ©èœå•"""
    response = [
        "ğŸ¤– **AIèŠå¤©åŠŸèƒ½è¯¦ç»†å¸®åŠ©**",
        "",
        "ğŸ“ **åŸºç¡€èŠå¤©**",
        "â€¢ ai + é—®é¢˜å†…å®¹ - æ™ºèƒ½é—®ç­”ï¼ˆè‡ªåŠ¨è¯†åˆ«ï¼Œä¸éœ€è¦@æœºå™¨äººï¼‰",
        "â€¢ ç¤ºä¾‹ï¼šai ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "",
        "ğŸ”„ **æ¨¡å‹ç®¡ç†**",
        "â€¢ åˆ‡æ¢åƒé—® - åˆ‡æ¢åˆ°åƒé—®æ¨¡å‹ï¼ˆé«˜æ€§èƒ½ä¸­æ–‡ç¼–ç æ¨¡å‹ï¼‰",
        "â€¢ åˆ‡æ¢gpt - åˆ‡æ¢åˆ°GPTæ¨¡å‹ï¼ˆé€šç”¨è¯­è¨€æ¨¡å‹ï¼‰",
        "â€¢ åˆ‡æ¢deepseek - åˆ‡æ¢åˆ°DeepSeekæ¨¡å‹ï¼ˆä¸“ä¸šç¼–ç¨‹æ¨¡å‹ï¼‰",
        "â€¢ å½“å‰æ¨¡å‹ - æŸ¥çœ‹å½“å‰ä½¿ç”¨çš„æ¨¡å‹",
        "â€¢ æ¨¡å‹åˆ—è¡¨ - æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹",
        "â€¢ é‡ç½®æ¨¡å‹ - é‡ç½®åˆ°é»˜è®¤æ¨¡å‹ï¼ˆåƒé—®ï¼‰",
        "",
        "ğŸ§¹ **å¯¹è¯ç®¡ç†**",
        "â€¢ æ¸…ç†å†å² - æ¸…ç†æ‚¨çš„å¯¹è¯å†å²ï¼ˆé‡ç½®å½“å‰ä¼šè¯ï¼‰",
        "",
        "ğŸ’¡ **ä½¿ç”¨æç¤º**",
        "â€¢ æ‰€æœ‰å‘½ä»¤æ”¯æŒç›´æ¥å‘é€æˆ–å¸¦/å‰ç¼€å‘é€",
        "â€¢ ä¾‹å¦‚ï¼š'åˆ‡æ¢åƒé—®' æˆ– '/åˆ‡æ¢åƒé—®' å‡å¯è§¦å‘",
        "â€¢ æ¨¡å‹åˆ‡æ¢åä¼šè‡ªåŠ¨åº”ç”¨äºåç»­çš„æ‰€æœ‰å¯¹è¯",
        "",
        "ğŸ”§ **æ•…éšœæ’é™¤**",
        "â€¢ å¦‚æœæ— æ³•è·å–å›å¤ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥",
        "â€¢ è¾“å…¥é”™è¯¯æˆ–ä¸æ”¯æŒçš„å‘½ä»¤å°†ä¸ä¼šè§¦å‘å“åº”"
    ]
    
    await bot.send(event, "\n".join(response))

async def handle_switch_qwen(bot, event):
    """åˆ‡æ¢åˆ°åƒé—®æ¨¡å‹"""
    global current_model
    current_model = "qwen3-coder:480b-cloud"
    logger.info(f"æ¨¡å‹å·²åˆ‡æ¢ä¸º: {current_model}")
    await bot.send(event, f"âœ… æ¨¡å‹å·²åˆ‡æ¢ä¸º: åƒé—® (qwen3-coder:480b-cloud)")

async def handle_switch_gpt(bot, event):
    """åˆ‡æ¢åˆ°GPTæ¨¡å‹"""
    global current_model
    current_model = "gpt-oss:120b-cloud"
    logger.info(f"æ¨¡å‹å·²åˆ‡æ¢ä¸º: {current_model}")
    await bot.send(event, f"âœ… æ¨¡å‹å·²åˆ‡æ¢ä¸º: GPT (gpt-oss:120b-cloud)")

async def handle_switch_deepseek(bot, event):
    """åˆ‡æ¢åˆ°DeepSeekæ¨¡å‹"""
    global current_model
    current_model = "deepseek-v3.1:671b-cloud"
    logger.info(f"æ¨¡å‹å·²åˆ‡æ¢ä¸º: {current_model}")
    await bot.send(event, f"âœ… æ¨¡å‹å·²åˆ‡æ¢ä¸º: DeepSeek (deepseek-v3.1:671b-cloud)")

async def handle_show_current_model(bot, event):
    """æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
    model_name = {
        "qwen3-coder:480b-cloud": "åƒé—®",
        "gpt-oss:120b-cloud": "GPT",
        "deepseek-v3.1:671b-cloud": "DeepSeek"
    }.get(current_model, current_model)
    await bot.send(event, f"å½“å‰ä½¿ç”¨çš„æ¨¡å‹: {model_name} ({current_model})")

async def handle_ollama_chat(bot, event):
    """å¤„ç†èŠå¤©æ¶ˆæ¯"""
    # è·å–ç”¨æˆ·å‘é€çš„æ¶ˆæ¯
    message = str(event.message)
    message_text = message.strip()
    
    # å®šä¹‰æ‰€æœ‰éœ€è¦æ’é™¤çš„å‘½ä»¤å…³é”®è¯åˆ—è¡¨
    COMMAND_KEYWORDS = [
        "sjbs", "sjhs", "sjmt", "sjecy", "sjsk",  # éšæœºå›¾ç‰‡å‘½ä»¤
        "åˆ‡æ¢åƒé—®", "åˆ‡æ¢gpt", "åˆ‡æ¢deepseek",    # æ¨¡å‹åˆ‡æ¢å‘½ä»¤
        "å½“å‰æ¨¡å‹", "æ¨¡å‹åˆ—è¡¨", "ollamaå¸®åŠ©",      # æ¨¡å‹ä¿¡æ¯å‘½ä»¤
        "æ¸…ç†å†å²", "é‡ç½®æ¨¡å‹",                    # å¯¹è¯ç®¡ç†å‘½ä»¤
        "å¸®åŠ©", "æµ‹è¯•é»„å†"                          # å…¶ä»–å‘½ä»¤
    ]
    
    # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦ä¸ºä»»ä½•å·²æ³¨å†Œçš„å‘½ä»¤ï¼Œå¦‚æœæ˜¯åˆ™ä¸å¤„ç†ï¼Œè®©å‘½ä»¤å¤„ç†å™¨å¤„ç†
    if message_text in COMMAND_KEYWORDS:
        logger.info(f"æ¶ˆæ¯'{message_text}'è¢«è¯†åˆ«ä¸ºå‘½ä»¤ï¼Œè·³è¿‡å¤„ç†ï¼Œäº¤ç»™å‘½ä»¤å¤„ç†å™¨")
        return
    
    # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦ä»¥"ai+ç©ºæ ¼"å¼€å¤´
    if not is_ai_prefix(message_text):
        logger.info(f"æ¶ˆæ¯'{message_text}'ä¸ä»¥'ai 'å¼€å¤´ï¼Œè·³è¿‡å¤„ç†")
        return
    
    # æå–å®é™…é—®é¢˜ï¼ˆç§»é™¤"ai "å‰ç¼€ï¼Œæ³¨æ„åŒ…å«ç©ºæ ¼ï¼‰
    question = message_text[3:].strip()
    # å¦‚æœç§»é™¤å‰ç¼€åæ¶ˆæ¯ä¸ºç©ºï¼Œä¸å¤„ç†
    if not question:
        return
    
    logger.info(f"æ”¶åˆ°AIé—®é¢˜: {question}")
    
    # è·å–ç”¨æˆ·ID
    user_id = event.get_user_id()
    
    # åˆå§‹åŒ–ç”¨æˆ·çš„å¯¹è¯å†å²
    if user_id not in conversation_histories:
        conversation_histories[user_id] = []
        logger.info(f"åˆå§‹åŒ–ç”¨æˆ· {user_id} çš„å¯¹è¯å†å²")
    
    try:
        logger.info(f"æ”¶åˆ°ç”¨æˆ· {user_id} çš„AIé—®é¢˜: {question}")
        
        # è°ƒç”¨Ollama APIè·å–å›å¤
        response_text = await get_ollama_response(question, user_id)
        
        if response_text:
            logger.info(f"è·å–Ollamaå›å¤æˆåŠŸï¼Œç”¨æˆ· {user_id}")
            # å¦‚æœå›å¤å†…å®¹è¿‡é•¿ï¼Œåˆ†æ®µå‘é€
            if len(response_text) > 2000:
                chunks = [response_text[i:i+2000] for i in range(0, len(response_text), 2000)]
                for chunk in chunks:
                    await bot.send(event, chunk)
                    await asyncio.sleep(1)  # é¿å…æ¶ˆæ¯å‘é€è¿‡å¿«
            else:
                await bot.send(event, response_text)
        else:
            await bot.send(event, "âŒ è·å–Ollamaå›å¤å¤±è´¥ï¼Œè¯·ç¨åå†è¯•")
            
    except Exception as e:
        logger.error(f"OllamaèŠå¤©å¤„ç†å¼‚å¸¸: {type(e).__name__}: {str(e)}")
        await bot.send(event, f"âŒ èŠå¤©å¤„ç†å¼‚å¸¸: {type(e).__name__}: {str(e)}")

async def get_ollama_response(message: str, user_id: str) -> str:
    """è°ƒç”¨Ollama APIè·å–å›å¤ - ä½¿ç”¨å®˜æ–¹Pythonå®¢æˆ·ç«¯åº“"""
    global current_model
    
    # æ›´æ–°å¯¹è¯å†å²
    conversation_histories[user_id].append({"role": "user", "content": message})
    
    logger.info(f"è°ƒç”¨Ollama API: æ¨¡å‹={current_model}, ç”¨æˆ·ID={user_id}")
    logger.info(f"Ollamaä¸»æœºåœ°å€: {OLLAMA_HOST}")
    logger.info(f"æ¶ˆæ¯å†…å®¹: {message}")
    
    try:
        # ä½¿ç”¨Ollamaå®˜æ–¹Pythonå®¢æˆ·ç«¯åº“
        client = Client(
            host=OLLAMA_HOST,
            headers={'Authorization': f'Bearer {ollama_api_key}'}
        )
        
        # åŒæ­¥å‡½æ•°éœ€è¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è¿è¡Œï¼Œä½¿ç”¨loop.run_in_executor
        loop = asyncio.get_event_loop()
        
        # ä¿®å¤ï¼šåªä¼ é€’æ¨¡æ‹ŸClientç±»æ”¯æŒçš„ä¸‰ä¸ªå‚æ•°
        response = await loop.run_in_executor(
            None, 
            lambda: client.chat(
                model=current_model, 
                messages=conversation_histories[user_id],
                stream=False
            )
        )
        
        logger.info(f"APIå“åº”æˆåŠŸ: {response}")
        
        if "message" in response:
            response_text = response["message"]["content"]
            # å°†åŠ©æ‰‹å›å¤æ·»åŠ åˆ°å¯¹è¯å†å²
            conversation_histories[user_id].append({"role": "assistant", "content": response_text})
            return response_text
        else:
            logger.error(f"Ollama APIè¿”å›æ— æ•ˆå“åº”: {response}")
            return ""
    
    except ImportError as e:
        logger.error(f"æœªå®‰è£…ollama Pythonåº“: {str(e)}")
        return "âŒ è¯·å…ˆå®‰è£…ollama Pythonåº“: pip install ollama"
    except Exception as e:
        logger.error(f"Ollama APIè°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {str(e)}")
        # å°è¯•è¯†åˆ«å¸¸è§é”™è¯¯
        if "No API key provided" in str(e) or "Unauthorized" in str(e):
            return "âŒ APIå¯†é’¥é”™è¯¯æˆ–æœªé…ç½®ï¼Œè¯·æ£€æŸ¥OLLAMA_API_KEYç¯å¢ƒå˜é‡"
        elif "Connection refused" in str(e) or "Cannot connect" in str(e):
            return "âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        elif "Model not found" in str(e):
            return "âŒ æ¨¡å‹æœªæ‰¾åˆ°ï¼Œè¯·ç¡®è®¤æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®"
        else:
            return f"âŒ APIè°ƒç”¨é”™è¯¯: {str(e)}"

# æ¸…ç†å¯¹è¯å†å²å‘½ä»¤å·²åœ¨ä¸Šæ–¹å®šä¹‰

async def handle_clear_history(bot, event):
    """æ¸…ç†æŒ‡å®šç”¨æˆ·çš„å¯¹è¯å†å²"""
    # è·å–ç”¨æˆ·ID
    user_id = event.get_user_id()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºè¶…çº§ç”¨æˆ·
    if user_id in config.superusers:
        # è·å–å‘½ä»¤å‚æ•°ï¼Œå°è¯•è§£æè¦æ¸…ç†çš„ç”¨æˆ·ID
        args = str(event.message).strip()
        if args:
            # å°è¯•å°†å‚æ•°è§£æä¸ºç”¨æˆ·ID
            target_user_id = args
        else:
            # æ²¡æœ‰å‚æ•°ï¼Œæ¸…ç†æ‰€æœ‰ç”¨æˆ·çš„å¯¹è¯å†å²
            conversation_histories.clear()
            logger.info(f"è¶…çº§ç”¨æˆ· {user_id} æ¸…ç†äº†æ‰€æœ‰ç”¨æˆ·çš„å¯¹è¯å†å²")
            await bot.send(event, "âœ… å·²æ¸…ç†æ‰€æœ‰ç”¨æˆ·çš„å¯¹è¯å†å²")
            return
    else:
        # æ™®é€šç”¨æˆ·åªèƒ½æ¸…ç†è‡ªå·±çš„å¯¹è¯å†å²
        target_user_id = user_id
    
    # æ¸…ç†æŒ‡å®šç”¨æˆ·çš„å¯¹è¯å†å²
    if target_user_id in conversation_histories:
        del conversation_histories[target_user_id]
        logger.info(f"ç”¨æˆ· {user_id} æ¸…ç†äº†ç”¨æˆ· {target_user_id} çš„å¯¹è¯å†å²")
        if user_id == target_user_id:
            await bot.send(event, "âœ… å·²æ¸…ç†æ‚¨çš„å¯¹è¯å†å²")
        else:
            await bot.send(event, f"âœ… å·²æ¸…ç†ç”¨æˆ· {target_user_id} çš„å¯¹è¯å†å²")
    else:
        if user_id == target_user_id:
            await bot.send(event, "âŒ æ‚¨æ²¡æœ‰å¯¹è¯å†å²å¯ä»¥æ¸…ç†")
        else:
            await bot.send(event, f"âŒ ç”¨æˆ· {target_user_id} æ²¡æœ‰å¯¹è¯å†å²å¯ä»¥æ¸…ç†")

# é‡ç½®æ¨¡å‹å‘½ä»¤å·²åœ¨ä¸Šæ–¹å®šä¹‰

async def handle_reset_model(bot, event):
    """é‡ç½®æ¨¡å‹åˆ°é»˜è®¤å€¼"""
    global current_model
    current_model = DEFAULT_MODEL
    logger.info(f"æ¨¡å‹å·²é‡ç½®ä¸ºé»˜è®¤å€¼: {current_model}")
    await bot.send(event, f"âœ… æ¨¡å‹å·²é‡ç½®ä¸ºé»˜è®¤å€¼: åƒé—® (qwen3-coder:480b-cloud)")