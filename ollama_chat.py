"""Ollamaäº‘æ¨¡å‹èŠå¤©æ’ä»¶"""

import json
import asyncio
from datetime import datetime
from nonebot import on_command, on_message, get_driver, logger
from nonebot.adapters.onebot.v11 import Bot, Event, MessageSegment
from nonebot.rule import to_me
from nonebot.permission import SUPERUSER
from nonebot.plugin import PluginMetadata

# å¯¼å…¥Ollamaçš„Pythonå®¢æˆ·ç«¯åº“ - è¿™æ˜¯å®˜æ–¹æ¨èçš„è°ƒç”¨æ–¹å¼
try:
    from ollama import Client
except ImportError:
    logger.error("æœªå®‰è£…ollama Pythonåº“ï¼Œè¯·è¿è¡Œ: pip install ollama")
    # ä¸ºäº†é˜²æ­¢å¯¼å…¥å¤±è´¥å¯¼è‡´æ•´ä¸ªæ’ä»¶æ— æ³•åŠ è½½ï¼Œåˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„Clientç±»
    class Client:
        def __init__(self, host=None, headers=None):
            self.host = host
            self.headers = headers
        def chat(self, model, messages, stream=False):
            raise ImportError("æœªå®‰è£…ollama Pythonåº“")

__plugin_meta__ = PluginMetadata(
    name="Ollamaäº‘æ¨¡å‹èŠå¤©",
    description="ä½¿ç”¨Ollamaäº‘æ¨¡å‹è¿›è¡ŒèŠå¤©ï¼Œæ”¯æŒåˆ‡æ¢ä¸åŒæ¨¡å‹",
    usage="@æœºå™¨äºº å‘é€æ¶ˆæ¯è¿›è¡ŒèŠå¤©\n/åˆ‡æ¢åƒé—® åˆ‡æ¢åˆ°qwen3-coder:480b-cloudæ¨¡å‹\n/åˆ‡æ¢gpt åˆ‡æ¢åˆ°gpt-oss:120b-cloudæ¨¡å‹\n/åˆ‡æ¢deepseek åˆ‡æ¢åˆ°deepseek-v3.1:671b-cloudæ¨¡å‹\n/å½“å‰æ¨¡å‹ æŸ¥çœ‹å½“å‰ä½¿ç”¨çš„æ¨¡å‹",
)

# ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
config = get_driver().config
ollama_api_key = getattr(config, "ollama_api_key", "")

# é»˜è®¤æ¨¡å‹ - æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œäº‘ç«¯æ¨¡å‹éœ€è¦å¸¦-cloudåç¼€
DEFAULT_MODEL = "qwen3-coder:480b-cloud"
# å½“å‰ä½¿ç”¨çš„æ¨¡å‹
current_model = DEFAULT_MODEL
# å¯¹è¯å†å²ç¼“å­˜
conversation_histories = {}
# Ollama APIä¸»æœºåœ°å€
OLLAMA_HOST = "https://ollama.com"

# åœ¨æ–‡ä»¶é¡¶éƒ¨çš„å‘½ä»¤å®šä¹‰éƒ¨åˆ†æ·»åŠ æ–°å‘½ä»¤
# å‘½ä»¤å®šä¹‰
switch_qwen = on_command("åˆ‡æ¢åƒé—®", priority=10, block=True)
switch_gpt = on_command("åˆ‡æ¢gpt", priority=10, block=True)
switch_deepseek = on_command("åˆ‡æ¢deepseek", priority=10, block=True)
show_current_model = on_command("å½“å‰æ¨¡å‹", priority=10, block=True)
model_list = on_command("æ¨¡å‹åˆ—è¡¨", priority=10, block=True)
ollama_help = on_command("ollamaå¸®åŠ©", aliases={"Ollamaå¸®åŠ©", "ollamaèœå•", "Ollamaèœå•"}, priority=10, block=True)

# èŠå¤©æ¶ˆæ¯å¤„ç†
ollama_chat = on_message(rule=to_me(), priority=15, block=False)

# å¯ç”¨æ¨¡å‹åˆ—è¡¨
available_models = [
    {"name": "qwen3-coder:480b-cloud", "chinese_name": "åƒé—®", "description": "é«˜æ€§èƒ½ä¸­æ–‡ç¼–ç æ¨¡å‹"},
    {"name": "gpt-oss:120b-cloud", "chinese_name": "GPT", "description": "é€šç”¨è¯­è¨€æ¨¡å‹"},
    {"name": "deepseek-v3.1:671b-cloud", "chinese_name": "DeepSeek", "description": "ä¸“ä¸šç¼–ç¨‹æ¨¡å‹"}
]

# æ·»åŠ æ¨¡å‹åˆ—è¡¨å‘½ä»¤å¤„ç†å‡½æ•°
@model_list.handle()
async def handle_model_list(bot: Bot, event: Event):
    """æ˜¾ç¤ºå¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    response = "ğŸ“‹ **å¯ç”¨æ¨¡å‹åˆ—è¡¨**\n\n"
    for i, model in enumerate(available_models, 1):
        is_current = " âœ…" if model["name"] == current_model else ""
        response += f"{i}. {model['chinese_name']} ({model['name']}){is_current}\n"
        response += f"   ç®€ä»‹: {model['description']}\n\n"
    await bot.send(event, response)

# æ·»åŠ æ’ä»¶å¸®åŠ©èœå•å‘½ä»¤å¤„ç†å‡½æ•°
@ollama_help.handle()
async def handle_ollama_help(bot: Bot, event: Event):
    """æ˜¾ç¤ºOllamaèŠå¤©æ’ä»¶çš„å¸®åŠ©èœå•"""
    response = "ğŸ¤– **OllamaèŠå¤©æ’ä»¶å¸®åŠ©èœå•**\n\n"
    response += "ğŸ“ **èŠå¤©åŠŸèƒ½**\n"
    response += "@æœºå™¨äºº + æ¶ˆæ¯å†…å®¹\n\n"
    
    response += "ğŸ”„ **æ¨¡å‹ç®¡ç†**\n"
    response += "/åˆ‡æ¢åƒé—® - åˆ‡æ¢åˆ°åƒé—®æ¨¡å‹\n"
    response += "/åˆ‡æ¢gpt - åˆ‡æ¢åˆ°GPTæ¨¡å‹\n"
    response += "/åˆ‡æ¢deepseek - åˆ‡æ¢åˆ°DeepSeekæ¨¡å‹\n"
    response += "/å½“å‰æ¨¡å‹ - æŸ¥çœ‹å½“å‰ä½¿ç”¨çš„æ¨¡å‹\n"
    response += "/æ¨¡å‹åˆ—è¡¨ - æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹\n"
    response += "/é‡ç½®æ¨¡å‹ - é‡ç½®åˆ°é»˜è®¤æ¨¡å‹\n\n"
    
    response += "ğŸ§¹ **å¯¹è¯ç®¡ç†**\n"
    response += "/æ¸…ç†å¯¹è¯ - æ¸…ç†æ‚¨çš„å¯¹è¯å†å²\n"
    response += "/æ¸…ç†å¯¹è¯ å…¨éƒ¨ - è¶…çº§ç”¨æˆ·æ¸…ç†æ‰€æœ‰å¯¹è¯å†å²\n\n"
    
    response += "â„¹ï¸ **å¸®åŠ©ä¿¡æ¯**\n"
    response += "/ollamaå¸®åŠ© - æ˜¾ç¤ºæ­¤å¸®åŠ©èœå•\n"
    
    await bot.send(event, response)

@switch_qwen.handle()
async def handle_switch_qwen(bot: Bot, event: Event):
    """åˆ‡æ¢åˆ°åƒé—®æ¨¡å‹"""
    global current_model
    current_model = "qwen3-coder:480b-cloud"
    logger.info(f"æ¨¡å‹å·²åˆ‡æ¢ä¸º: {current_model}")
    await bot.send(event, f"âœ… æ¨¡å‹å·²åˆ‡æ¢ä¸º: åƒé—® (qwen3-coder:480b-cloud)")

@switch_gpt.handle()
async def handle_switch_gpt(bot: Bot, event: Event):
    """åˆ‡æ¢åˆ°GPTæ¨¡å‹"""
    global current_model
    current_model = "gpt-oss:120b-cloud"
    logger.info(f"æ¨¡å‹å·²åˆ‡æ¢ä¸º: {current_model}")
    await bot.send(event, f"âœ… æ¨¡å‹å·²åˆ‡æ¢ä¸º: GPT (gpt-oss:120b-cloud)")

@switch_deepseek.handle()
async def handle_switch_deepseek(bot: Bot, event: Event):
    """åˆ‡æ¢åˆ°DeepSeekæ¨¡å‹"""
    global current_model
    current_model = "deepseek-v3.1:671b-cloud"
    logger.info(f"æ¨¡å‹å·²åˆ‡æ¢ä¸º: {current_model}")
    await bot.send(event, f"âœ… æ¨¡å‹å·²åˆ‡æ¢ä¸º: DeepSeek (deepseek-v3.1:671b-cloud)")

@show_current_model.handle()
async def handle_show_current_model(bot: Bot, event: Event):
    """æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
    model_name = {
        "qwen3-coder:480b-cloud": "åƒé—®",
        "gpt-oss:120b-cloud": "GPT",
        "deepseek-v3.1:671b-cloud": "DeepSeek"
    }.get(current_model, current_model)
    await bot.send(event, f"å½“å‰ä½¿ç”¨çš„æ¨¡å‹: {model_name} ({current_model})")

@ollama_chat.handle()
async def handle_ollama_chat(bot: Bot, event: Event):
    """å¤„ç†èŠå¤©æ¶ˆæ¯"""
    # è·å–ç”¨æˆ·å‘é€çš„æ¶ˆæ¯ï¼Œå»é™¤@æœºå™¨äººçš„éƒ¨åˆ†
    message = str(event.message)
    # å¦‚æœæ¶ˆæ¯ä¸ºç©ºæˆ–ä»…åŒ…å«å‘½ä»¤ï¼Œä¸å¤„ç†
    if not message or message.strip() in ["æ¸…ç†", "é‡ç½®"]:
        return
    
    # è·å–ç”¨æˆ·ID
    user_id = event.get_user_id()
    
    # åˆå§‹åŒ–ç”¨æˆ·çš„å¯¹è¯å†å²
    if user_id not in conversation_histories:
        conversation_histories[user_id] = []
        logger.info(f"åˆå§‹åŒ–ç”¨æˆ· {user_id} çš„å¯¹è¯å†å²")
    
    try:
        logger.info(f"æ”¶åˆ°ç”¨æˆ· {user_id} çš„æ¶ˆæ¯: {message}")
        
        # è°ƒç”¨Ollama APIè·å–å›å¤
        response_text = await get_ollama_response(message, user_id)
        
        if response_text:
            logger.info(f"è·å–Ollamaå›å¤æˆåŠŸï¼Œç”¨æˆ· {user_id}")
            # å¦‚æœå›å¤å†…å®¹è¿‡é•¿ï¼Œåˆ†æ®µå‘é€
            if len(response_text) > 2000:
                chunks = [response_text[i:i+2000] for i in range(0, len(response_text), 2000)]
                for chunk in chunks:
                    await bot.send(event, chunk)
                    await asyncio.sleep(1)  # é¿å…æ¶ˆæ¯å‘é€è¿‡å¿«
            else:
                await bot.send(event, response_text)
        else:
            await bot.send(event, "âŒ è·å–Ollamaå›å¤å¤±è´¥ï¼Œè¯·ç¨åå†è¯•")
            
    except Exception as e:
        logger.error(f"OllamaèŠå¤©å¤„ç†å¼‚å¸¸: {type(e).__name__}: {str(e)}")
        await bot.send(event, f"âŒ èŠå¤©å¤„ç†å¼‚å¸¸: {type(e).__name__}: {str(e)}")

async def get_ollama_response(message: str, user_id: str) -> str:
    """è°ƒç”¨Ollama APIè·å–å›å¤ - ä½¿ç”¨å®˜æ–¹Pythonå®¢æˆ·ç«¯åº“"""
    global current_model
    
    # æ›´æ–°å¯¹è¯å†å²
    conversation_histories[user_id].append({"role": "user", "content": message})
    
    logger.info(f"è°ƒç”¨Ollama API: æ¨¡å‹={current_model}, ç”¨æˆ·ID={user_id}")
    logger.info(f"Ollamaä¸»æœºåœ°å€: {OLLAMA_HOST}")
    logger.info(f"æ¶ˆæ¯å†…å®¹: {message}")
    
    try:
        # ä½¿ç”¨Ollamaå®˜æ–¹Pythonå®¢æˆ·ç«¯åº“
        client = Client(
            host=OLLAMA_HOST,
            headers={'Authorization': f'Bearer {ollama_api_key}'}
        )
        
        # åŒæ­¥å‡½æ•°éœ€è¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è¿è¡Œï¼Œä½¿ç”¨loop.run_in_executor
        loop = asyncio.get_event_loop()
        
        # ä¿®å¤ï¼šåªä¼ é€’æ¨¡æ‹ŸClientç±»æ”¯æŒçš„ä¸‰ä¸ªå‚æ•°
        response = await loop.run_in_executor(
            None, 
            lambda: client.chat(
                model=current_model, 
                messages=conversation_histories[user_id],
                stream=False
            )
        )
        
        logger.info(f"APIå“åº”æˆåŠŸ: {response}")
        
        if "message" in response:
            response_text = response["message"]["content"]
            # å°†åŠ©æ‰‹å›å¤æ·»åŠ åˆ°å¯¹è¯å†å²
            conversation_histories[user_id].append({"role": "assistant", "content": response_text})
            return response_text
        else:
            logger.error(f"Ollama APIè¿”å›æ— æ•ˆå“åº”: {response}")
            return ""
    
    except ImportError as e:
        logger.error(f"æœªå®‰è£…ollama Pythonåº“: {str(e)}")
        return "âŒ è¯·å…ˆå®‰è£…ollama Pythonåº“: pip install ollama"
    except Exception as e:
        logger.error(f"Ollama APIè°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {str(e)}")
        # å°è¯•è¯†åˆ«å¸¸è§é”™è¯¯
        if "No API key provided" in str(e) or "Unauthorized" in str(e):
            return "âŒ APIå¯†é’¥é”™è¯¯æˆ–æœªé…ç½®ï¼Œè¯·æ£€æŸ¥OLLAMA_API_KEYç¯å¢ƒå˜é‡"
        elif "Connection refused" in str(e) or "Cannot connect" in str(e):
            return "âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        elif "Model not found" in str(e):
            return "âŒ æ¨¡å‹æœªæ‰¾åˆ°ï¼Œè¯·ç¡®è®¤æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®"
        else:
            return f"âŒ APIè°ƒç”¨é”™è¯¯: {str(e)}"

# æ¸…ç†å¯¹è¯å†å²çš„å‘½ä»¤
clear_history = on_command("æ¸…ç†å†å²", aliases={"é‡ç½®å¯¹è¯"}, priority=10, block=True)

@clear_history.handle()
async def handle_clear_history(bot: Bot, event: Event):
    """æ¸…ç†æŒ‡å®šç”¨æˆ·çš„å¯¹è¯å†å²"""
    # è·å–ç”¨æˆ·ID
    user_id = event.get_user_id()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºè¶…çº§ç”¨æˆ·
    if user_id in config.superusers:
        # è·å–å‘½ä»¤å‚æ•°ï¼Œå°è¯•è§£æè¦æ¸…ç†çš„ç”¨æˆ·ID
        args = str(event.message).strip()
        if args:
            # å°è¯•å°†å‚æ•°è§£æä¸ºç”¨æˆ·ID
            target_user_id = args
        else:
            # æ²¡æœ‰å‚æ•°ï¼Œæ¸…ç†æ‰€æœ‰ç”¨æˆ·çš„å¯¹è¯å†å²
            conversation_histories.clear()
            logger.info(f"è¶…çº§ç”¨æˆ· {user_id} æ¸…ç†äº†æ‰€æœ‰ç”¨æˆ·çš„å¯¹è¯å†å²")
            await bot.send(event, "âœ… å·²æ¸…ç†æ‰€æœ‰ç”¨æˆ·çš„å¯¹è¯å†å²")
            return
    else:
        # æ™®é€šç”¨æˆ·åªèƒ½æ¸…ç†è‡ªå·±çš„å¯¹è¯å†å²
        target_user_id = user_id
    
    # æ¸…ç†æŒ‡å®šç”¨æˆ·çš„å¯¹è¯å†å²
    if target_user_id in conversation_histories:
        del conversation_histories[target_user_id]
        logger.info(f"ç”¨æˆ· {user_id} æ¸…ç†äº†ç”¨æˆ· {target_user_id} çš„å¯¹è¯å†å²")
        if user_id == target_user_id:
            await bot.send(event, "âœ… å·²æ¸…ç†æ‚¨çš„å¯¹è¯å†å²")
        else:
            await bot.send(event, f"âœ… å·²æ¸…ç†ç”¨æˆ· {target_user_id} çš„å¯¹è¯å†å²")
    else:
        if user_id == target_user_id:
            await bot.send(event, "âŒ æ‚¨æ²¡æœ‰å¯¹è¯å†å²å¯ä»¥æ¸…ç†")
        else:
            await bot.send(event, f"âŒ ç”¨æˆ· {target_user_id} æ²¡æœ‰å¯¹è¯å†å²å¯ä»¥æ¸…ç†")

# é‡ç½®æ¨¡å‹åˆ°é»˜è®¤å€¼çš„å‘½ä»¤
reset_model = on_command("é‡ç½®æ¨¡å‹", priority=10, block=True)

@reset_model.handle()
async def handle_reset_model(bot: Bot, event: Event):
    """é‡ç½®æ¨¡å‹åˆ°é»˜è®¤å€¼"""
    global current_model
    current_model = DEFAULT_MODEL
    logger.info(f"æ¨¡å‹å·²é‡ç½®ä¸ºé»˜è®¤å€¼: {current_model}")
    await bot.send(event, f"âœ… æ¨¡å‹å·²é‡ç½®ä¸ºé»˜è®¤å€¼: åƒé—® (qwen3-coder:480b-cloud)")